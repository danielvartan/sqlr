as.numeric()
contraints <- constraint %>%
dplyr::filter(any(constaint_id == constraints))
contraints <- constraint %>%
dplyr::filter(any(constraint_id == constraints))
contraints <- constraint %>%
dplyr::filter(constraint_id %in% constraints)
View(constraint)
View(contraints)
devtools::load_all(".")
constraint_set(x)
devtools::load_all(".")
constraint_set(x)
devtools::load_all(".")
constraint_set(x)
devtools::load_all(".")
write_query()
package = NULL
checkmate::assert_string(package, null.ok = TRUE)
if (!is_namespace_loaded("utils") ||
!is_namespace_loaded("magrittr")) {
stop("This function requires the 'utils' and 'magrittr' packages ",
'to run. You can install them by running: \n \n',
'install.packages("utils") \n',
'install.packages("magrittr")' , call. = FALSE)
}
if (is.null(package)) package <- get_package_name()
assert_namespace(package)
assert_data("search", package, alert = "gipso_2")
utils::data("search", package = package)
sheet_id <- sheet_id("search", package = package)
checkmate::assert_data_frame(search, min.rows = 1)
search <- search %>%
dplyr::select(source_provider:query, approved) %>%
dplyr::mutate(
source_provider = dplyr::case_when(
tolower(source_provider) == "apa psycnet" ~ "APA",
tolower(source_provider) == "ebscohost" ~ "EBSCO",
tolower(source_provider) == "web of science" ~ "WoS",
TRUE ~ source_provider
)
)
out <- character()
for (i in seq_len(nrow(search))) {
if (is.na(search$domain_set[i])) {
out <- out %>% append(as.character(NA))
} else {
out <- out %>% append(
query(domain_set(search$domain_set[i],
search$language[i],
package),
provider = search$source_provider[i],
constraint = constraint_set(search$constraint_set[i],
package),
print = FALSE, clipboard = FALSE)
)
}
}
search$constraint_set[i]
package
constraint_set(search$constraint_set[i],
package)
stop("'x' must conform to the pattern ",
"'^[0-9]+(\\s\\w+\\s[0-9]+)+$'", ", i.e., ",
"'x' must be a string like '1 AND 2' or '1 NOT 2 AND 3'. ",
"Please note that all characters that don't conform to the ",
"pattern '[^A-Za-z0-9 ]' are removed.",
call. = FALSE)
domain_set(search$domain_set[i],
search$language[i],
package)
search$domain_set[i]
stop("'x' must conform to the pattern ",
"'^[0-9]$|^[0-9]+(\\s\\w+\\s[0-9]+)+$'", ", i.e., ",
"'x' must be a string like '1', 1 AND 2', or '1 NOT 2 AND 3'. ",
"Please note that all characters that don't conform to the ",
"pattern '[^A-Za-z0-9 ]' are removed.",
call. = FALSE)
devtools::load_all(".")
out <- character()
for (i in seq_len(nrow(search))) {
if (is.na(search$domain_set[i])) {
out <- out %>% append(as.character(NA))
} else {
out <- out %>% append(
query(domain_set(search$domain_set[i],
search$language[i],
package),
provider = search$source_provider[i],
constraint = constraint_set(search$constraint_set[i],
package),
print = FALSE, clipboard = FALSE)
)
}
}
devtools::load_all(".")
constraint_set("1 AND (4 OR 5)")
devtools::load_all(".")
out <- character()
for (i in seq_len(nrow(search))) {
if (is.na(search$domain_set[i])) {
out <- out %>% append(as.character(NA))
} else {
out <- out %>% append(
query(domain_set(search$domain_set[i],
search$language[i],
package),
provider = search$source_provider[i],
constraint = constraint_set(search$constraint_set[i],
package),
print = FALSE, clipboard = FALSE)
)
}
}
printer(out)
x <- "This function requires the 'utils', 'magrittr', and "
x
"'googlesheets4' packages to run. "
stop("This function requires the 'utils', 'magrittr', and ",
"'googlesheets4' packages to run. ",
"You can install them by running: \n\n'",
'install.packages("utils") \n',
'install.packages("magrittr")' , call. = FALSE)
stop("This function requires the 'utils', 'magrittr', and ",
"'googlesheets4' packages to run. ",
"You can install them by running: \n\n",
'install.packages("utils") \n',
'install.packages("magrittr")' , call. = FALSE)
stop("This function requires the 'utils', 'magrittr', and ",
"'googlesheets4' packages to run. ",
"You can install them by running: \n\n",
'install.packages("utils") \n',
'install.packages("magrittr") \n',
'install.packages("googlesheets4") \n', call. = FALSE)
package = NULL
checkmate::assert_string(package, null.ok = TRUE)
if (!is_namespace_loaded("utils") ||
!is_namespace_loaded("magrittr") ||
!is_namespace_loaded("googlesheets4")) {
stop("This function requires the 'utils', 'magrittr', and ",
"'googlesheets4' packages to run. ",
"You can install them by running: \n\n",
'install.packages("utils") \n',
'install.packages("magrittr") \n',
'install.packages("googlesheets4") \n', call. = FALSE)
}
if (is.null(package)) package <- get_package_name()
assert_namespace(package)
assert_data("search", package, alert = "gipso_2")
utils::data("search", package = package)
sheet_id <- sheet_id("search", package = package)
checkmate::assert_data_frame(search, min.rows = 1)
search <- search %>%
dplyr::select(source_provider:query, approved) %>%
dplyr::mutate(
source_provider = dplyr::case_when(
tolower(source_provider) == "apa psycnet" ~ "APA",
tolower(source_provider) == "ebscohost" ~ "EBSCO",
tolower(source_provider) == "web of science" ~ "WoS",
TRUE ~ source_provider
)
)
out <- character()
for (i in seq_len(nrow(search))) {
if (is.na(search$domain_set[i])) {
out <- out %>% append(as.character(NA))
} else {
out <- out %>% append(
query(domain_set(search$domain_set[i],
search$language[i],
package),
provider = search$source_provider[i],
constraint = constraint_set(search$constraint_set[i],
package),
print = FALSE, clipboard = FALSE)
)
}
}
out <- dplyr::tibble(query = out)
View(out)
sheets
sheets["search"]
args(googlesheets4::range_write)
devtools::load_all(".")
write_query()
data <- read_sheet("search")
debugonce(read_sheet)
data <- read_sheet("search")
sheets
ls()
data <- read_sheet("search")
devtools::load_all(".")
data <- read_sheet("search")
View(data)
keyword_set(1, "english")
tidy_keyword(keyword_set(1, "english"))
x <- 1
language <- "english"
stop("Some keywords from the domain ", single_quote_(x[i]),
"and language ", single_quote_(language),
"are missing after the keyword tidying process.")
i <_ 1
i <- 1
stop("Some keywords from the domain ", single_quote_(x[i]),
"and language ", single_quote_(language),
"are missing after the keyword tidying process.")
stop("Some keywords from the domain ", single_quote_(x[i]),
" and language ", single_quote_(language),
"are missing after the keyword tidying process.")
stop("Some keywords from the domain ", single_quote_(x[i]),
" and language ", single_quote_(language),
" are missing after the keyword tidying process.")
devtools::load_all(".")
write_query()
domain_set(1, "english")
domain_set("1", "english")
stop("Some keywords from the domain ", single_quote_(x[i]),
" and language ", single_quote_(language),
" are missing after the keyword tidying process.")
set <- keyword_set(as.numeric(x[i]), language, package)
tidy <- tidy_keyword(set)
keyword_set(as.numeric(x[i]), language, package)
package = NULL
keyword_set(as.numeric(x[i]), language, package)
set <- keyword_set(as.numeric(x[i]), language, package)
tidy <- tidy_keyword(set)
length(tidy)
length(set)
length(set) == length(tidy)
set <- keyword_set(as.numeric(x[i]), language, package)
tidy <- tidy_keyword(set)
if (length(set) == length(tidy)) {
stop("Some keywords from the domain ", single_quote_(x[i]),
" and language ", single_quote_(language),
" are missing after the keyword tidying process.")
}
devtools::load_all(".")
write_query()
x <- 2
language <- "portuguese"
set <- keyword_set(as.numeric(x[i]), language, package)
tidy <- tidy_keyword(set)
set
tidy
devtools::load_all(".")
write_sheet()
devtools::load_all(".")
message("\n", "Don't forget to run 'devtools::load_all()' ",
"(Ctrl + Shift + L).")
devtools::load_all(".")
write_query()
devtools::load_all(".")
?write_query
devtools::load_all(".")
write_query()
raw_data()
data(package = "sqlr")
View(search)
class(search$start)
class(search$start)[1]
search$start[1]
lubridate::as_datetime(search$start[1])
lubridate::as_datetime(search$start[1])
synthesisr::code_lookup
dir(system.file("extdata/", package = "sqlr"))
dir(system.file("extdata", package = "sqlr"))
devtools::load_all(".")
alert(test, test1, test2)
alert
debugonce(alert)
alert(test, test1, test2)
assert_has_length(1)
assert_has_length(list(1 = "a"))
assert_has_length(list(cs = "a"))
alert("test", "test1", "test2")
alert("test", "test1", "test2", "\n")
alert("Type of files:\n")
dir(system.file("extdata", package = "sqlr"))
alert("File types:\n")
dir(system.file("extdata", package = "sqlr"))
package <- "sqlr"
type <- "search_history"
path <- paste0("extdata/", type)
dir(system.file(path, package = package))
index <- list(
citation = list(name = "Citation",
path = "extdata/citation"),
search_history = list(name = "Search history",
path = "extdata/search_history")
)
index["citation"]
alert(index[type]$name, " files:", "\n")
index[type]$name
index[type]
index[type]["name"]
index[type][["name"]]
index[[type]][["name"]]
index[[type]]$name
alert(index[[type]]$name, " files:", "\n")
alert(index[[type]]$name, " files:", "\n")
dir(system.file(index[[type]]$path, package = package))
devtools::load_all(".")
raw_data(0)
raw_data()
devtools::load_all(".")
raw_data()
raw_data("test")
raw_data("test", "citation")
devtools::load_all(".")
raw_data("citation")
raw_data("search_history")
test <- raw_data("search_history")
test
devtools::load_all(".")
raw_data("search_history")
raw_data("search_history", quiet = TRUE)
raw_data("search_history", "2019-02-12_search-history_ebscohost_en.pdf", quiet = TRUE)
raw_data("search_history", quiet = TRUE)
raw_data("search_history", quiet = FALSE)
stringr::str_subset(raw_data("citation"), "pubmed")
raw_data("citation")
pubmed_files <- stringr::str_subset(raw_data("citation"), "pubmed")
ebscohost_files <- stringr::str_subset(raw_data(), "ebscohost")
embase_files <- stringr::str_subset(raw_data(), "embase")
lilacs_es_files <- stringr::str_subset(raw_data(), "lilacs_es") # failed
lilacs_pt_files <- stringr::str_subset(raw_data(), "lilacs_pt") # failed
psycnet_files <- stringr::str_subset(raw_data(), "psycnet") # failed
scielo_es_files <- stringr::str_subset(raw_data(), "scielo_es")
scielo_pt_files <- stringr::str_subset(raw_data(), "scielo_pt")
scopus_files <- stringr::str_subset(raw_data(), "scopus")
wos_files <- stringr::str_subset(raw_data(), "web-of-science")
pubmed_files <- stringr::str_subset(citation_files, "pubmed")
ebscohost_files <- stringr::str_subset(citation_files, "ebscohost")
embase_files <- stringr::str_subset(citation_files, "embase")
lilacs_es_files <- stringr::str_subset(citation_files, "lilacs_es") # failed
lilacs_pt_files <- stringr::str_subset(citation_files, "lilacs_pt") # failed
psycnet_files <- stringr::str_subset(citation_files, "psycnet") # failed
scielo_es_files <- stringr::str_subset(citation_files, "scielo_es")
scielo_pt_files <- stringr::str_subset(citation_files, "scielo_pt")
scopus_files <- stringr::str_subset(citation_files, "scopus")
wos_files <- stringr::str_subset(citation_files, "web-of-science")
citation_files <- raw_data("citation", quiet = TRUE)
pubmed_files <- stringr::str_subset(citation_files, "pubmed")
ebscohost_files <- stringr::str_subset(citation_files, "ebscohost")
embase_files <- stringr::str_subset(citation_files, "embase")
lilacs_es_files <- stringr::str_subset(citation_files, "lilacs_es") # failed
lilacs_pt_files <- stringr::str_subset(citation_files, "lilacs_pt") # failed
psycnet_files <- stringr::str_subset(citation_files, "psycnet") # failed
scielo_es_files <- stringr::str_subset(citation_files, "scielo_es")
scielo_pt_files <- stringr::str_subset(citation_files, "scielo_pt")
scopus_files <- stringr::str_subset(citation_files, "scopus")
wos_files <- stringr::str_subset(citation_files, "web-of-science")
db <- "pubmed"
path <- "./inst/extdata/citation/"
files <- paste0(path, get(paste0(db, "_files")))
assign(db, synthesisr::read_ref(files))
db <- "pubmed"
path <- "./inst/extdata/citation/"
files <- paste0(path, get(paste0(db, "_files")))
?synthesisr::read_ref
files
getwd()
dir(path)
synthesisr::read_ref(raw_data("citation"), pubmed_files[1])
raw_data("citation", pubmed_files[1])
synthesisr::read_ref(raw_data("citation", pubmed_files[1]))
test <- synthesisr::read_ref(raw_data("citation", pubmed_files[1]))
View(test)
pubmed_files[1]
names(test)
synthesisr::code_lookup
View(synthesisr::code_lookup)
test <- synthesisr::read_ref(raw_data("citation", pubmed_files[1]), "pubmed")
View(synthesisr::code_lookup)
test <- synthesisr::read_ref(raw_data("citation", wos_files[1]), "wos")
View(test)
test <- synthesisr::read_ref(raw_data("citation", wos_files), "wos")
wos_files <- stringr::str_subset(citation_files, "web-of-science")
raw_data("citation", wos_files[1])
db <- "pubmed"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
files <- paste0(path, get(paste0(db, "_files")))
assign(db, synthesisr::read_ref(files, tag_namming))
pubmed_files
files
db <- "wos"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
files <- paste0(path, get(paste0(db, "_files")))
assign(db, synthesisr::read_ref(files, tag_namming))
db <- "wos"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
files <- paste0(path, get(paste0(db, "_files")))
assign(db, synthesisr::read_refs(files, tag_namming))
View(wos)
x <- c(a = 1, b = 2)
x
change_name(x, "")
devtools::load_all(".")
clear_names(x)
devtools::load_all(".")
clear_names(x)
x
clear_names(x)
assign(db, clear_names(db))
rownames(test)
rownames(test) <- NULL
View(test)
devtools::load_all(".")
db <- "wos"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
files <- paste0(path, get(paste0(db, "_files")))
assign(db, synthesisr::read_refs(files, tag_namming))
assign(db, clear_row_names(get(db)))
View(wos)
?synthesisr::find_duplicates
synthesisr::find_duplicates(get(db)$doi, method = "exact")
unique(synthesisr::find_duplicates(get(db)$doi, method = "exact"))
length(unique(synthesisr::find_duplicates(get(db)$doi, method = "exact")))
doi_match <- synthesisr::find_duplicates(get(db)$doi, method = "exact")
data_unique <- synthesisr::extract_unique_references(get(db), doi_match)
View(data_unique)
seq_along(doi_match)
doi_match[!(doi_match == seq_along(doi_match))]
db <- "ebscohost"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
files <- paste0(path, get(paste0(db, "_files")))
assign(db, synthesisr::read_refs(files, tag_namming))
assign(db, clear_row_names(get(db)))
doi_match <- synthesisr::find_duplicates(get(db)$doi, method = "exact")
doi_match[!(doi_match == seq_along(doi_match))]
View(ebscohost)
db <- "embase"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
files <- paste0(path, get(paste0(db, "_files")))
assign(db, synthesisr::read_refs(files, tag_namming))
assign(db, clear_row_names(get(db)))
db <- "lilacs_es"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
files <- paste0(path, get(paste0(db, "_files")))
assign(db, synthesisr::read_refs(files, tag_namming))
assign(db, clear_row_names(get(db)))
db <- "lilacs_pt"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
files <- paste0(path, get(paste0(db, "_files")))
assign(db, synthesisr::read_refs(files, tag_namming))
assign(db, clear_row_names(get(db)))
db <- "psycnet"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
files <- paste0(path, get(paste0(db, "_files")))
assign(db, synthesisr::read_refs(files, tag_namming))
assign(db, clear_row_names(get(db)))
?readLines
files <- character()
db <- "psycnet"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
i <- 1
files <- append(files, readLines(i, n = inf, ))
files <- append(files, readLines(i))
files <- append(files, readr::read_lines(i))
?readr::read_lines
i <- paste0(path, get(paste0(db, "_files")))[1]
files <- append(files, readr::read_lines(i))
files <- character()
for (i in paste0(path, get(paste0(db, "_files")))) {
files <- append(files, readLines(i))
files <- append(files, readLines(i))
?readLines
read <- readr::read_lines(i, skip = 4)
?synthesisr::read_refs()
tempfile()
db <- "psycnet"
path <- "./inst/extdata/citation/"
choices <- c("wos", "scopus", "ovid", "asp", "synthesir")
tag_namming <- if (db %in% choices) db else "best_guess"
if (db == "psycnet") {
data <- character()
for (i in paste0(path, get(paste0(db, "_files")))) {
read <- readr::read_lines(i, skip = 4)
data <- append(files, read)
}
files <- tempfile()
writeLines(data, files)
} else {
files <- paste0(path, get(paste0(db, "_files")))
}
files
assign(db, synthesisr::read_refs(files, tag_namming))
assign(db, clear_row_names(get(db)))
data
x <- c("1", "2", "")
x
c(x, "")
